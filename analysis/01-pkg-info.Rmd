---
title: Extracting package information 
output: html_document
---


```{r setup}
library(tidyverse)
library(lubridate)
library(rvest)
library(glue)
library(feasts)
library(cranlogs)
library(plotly)
library(scales)
library(lubridate)
knitr::opts_chunk$set(cache = TRUE,
                      cache.path = "cache/")
```

```{r pkg-info}
url <- "http://cran.rstudio.com/web/packages/packages.rds"
db <- readRDS(url(url)) %>% 
  as.data.frame()%>% 
  mutate(Description = str_replace_all(Description, "\n", " "),
         Description = str_squish(Description),
         Title = str_replace_all(Title, "\n", " "))
```


```{r db-format}
n_pkgs <- nrow(db)

dbl <- db %>% 
  # remove [aut,...], <mail>, or (...), etc
  mutate(author = str_remove_all(Author,                                 "(\\[.+\\]|\\<.+\\>|\\(.+\\))")) %>% 
  separate_rows(author, sep = ",") %>% 
  mutate(author = str_squish(author))

db_rstudio <- db %>% 
  filter(str_detect(tolower(Author), "rstudio"))
```


An alternative way to get the CRAN R-package names.
```{r cran-names}
cran_names <- rownames(available:::available_packages(repos = available:::default_cran_repos))
```

```{r}
# getting the total R packages download numbers from 1998
dd_start <- "2012-10-01"
dd_end <- Sys.Date() - 1

is_weekend <- function(date) {
  weekdays(date) %in% c("Saturday", "Sunday")
}

total_downloads <- cran_downloads(from = dd_start, to = dd_end) %>% 
  mutate(year = year(date),
         day = yday(date),
         weekend = is_weekend(date)) %>% 
  filter(row_number() <= n()-1)


# replace missing values with the nearest number 
total_downloads$count[total_downloads$count == 0] <- 21959



total_downloads %>%
  ggplot() + geom_line(aes(date, count))+
  geom_smooth(aes(date, count),stat = "smooth") 
```

```{r , cache=TRUE, message=FALSE,  results=FALSE, warning=FALSE, comment=FALSE}
#spilk_2014_dir <- download_RStudio_CRAN_data(START = '2014-11-17',END = '2014-11-17', log_folder="/Users/daidanyang/Documents/GitHub/paper-cran-category-classification/paper/Data")

# read .gz compressed files form local directory
#spilk_2014 <- read.csv("~/Documents/GitHub/paper-cran-category-classification/paper/Data/2014-11-17.csv.gz")

#save(spilk_2014, file = "spilk_2014.RData")

load(here::here("paper/spilk_2014.RData"))

country_2014 <- spilk_2014 %>%  
  group_by(country) %>% 
  count()

ID_2014 <- spilk_2014 %>%
  group_by(country, ip_id) %>%
  count()

pkg_ID_2014 <- spilk_2014 %>%
  group_by(country, ip_id, package) %>%
  count()

ido <- max(ID_2014$n) / sum(ID_2014$n)


download_141116 <-total_downloads %>% 
  filter(date == "2014-11-16")

download_141118 <-total_downloads %>% 
  filter(date == "2014-11-18")


spilk_2014 %>% 
  mutate(obsid = paste(country, ip_id, package)) %>% 
  pull(obsid) %>% 
  n_distinct()

```

```{r , cache=TRUE, message=FALSE,  results=FALSE, warning=FALSE, comment=FALSE}

#spilk_2018_dir <- download_RStudio_CRAN_data(START = '2018-10-21',END = '2018-10-21', log_folder="/Users/daidanyang/Documents/GitHub/paper-cran-category-classification/paper/Data")

# read .gz compressed files form local directory
#spilk_2018 <- read_RStudio_CRAN_data(spilk_2018_dir)

#save(spilk_2018, file = "spilk_2018.RData")

load("~/Documents/GitHub/paper-cran-category-classification/paper/spilk_2018.RData")

country_2018 <- spilk_2018 %>%  
  group_by(country) %>% 
  count()

ID_2018 <- spilk_2018 %>%  
  group_by(country,ip_id) %>% 
  count()

pkg_ID_18 <- spilk_2018 %>%  
  group_by(country,ip_id,package) %>% 
  count()


top15 <- pkg_ID_18[pkg_ID_18$n %in% tail(sort(pkg_ID_18$n),15),]

us_18 <- top15 %>% 
  group_by(country,package) %>% 
   summarise(n = sum(n))



```





```{r user-conference}
# adding the userR conference date 

conference_dates <- tribble(
  ~ name, ~ date,
  "UseR", "2004-05-20",
  "UseR", "2006-06-15",
  "UseR", "2007-08-08",
  "UseR", "2008-08-14",
  "UseR", "2009-07-08",
  "UseR", "2010-07-20",
  "UseR", "2011-08-16",
  "UseR", "2012-06-12",
  "UseR", "2013-06-10",
  "UseR", "2014-06-30",
  "UseR", "2015-06-30",
  "UseR", "2016-06-27",
  "UseR", "2017-07-04",
  "UseR", "2018-07-10",
  "UseR", "2019-07-09",
  "UseR", "2020-07-07",
  "UseR", "2021-07-05",
  "RStudio", "2017-01-11",
  "RStudio", "2018-01-31",
  "RStudio", "2019-01-15",
  "RStudio", "2020-01-27",
  "RStudio", "2021-03-03",
)

UseR <- dplyr::filter(conference_dates,name == "UseR")
Rstudio <-dplyr::filter(conference_dates,name == "RStudio")

download_user <- total_downloads %>%
  ggplot()  + geom_line(aes(date, count)) +
  geom_vline(xintercept = as.numeric(as.Date(UseR$date)), linetype="dotted", 
                color = "blue", size=0.1)+
  geom_vline(xintercept = as.numeric(as.Date(Rstudio$date)), linetype="dotted", 
                color = "red", size=0.1) 
   
ggplotly(download_user,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")
```



```{r}
summary(total_downloads$count)
total_downloads %>%
  ggplot()  + geom_line(aes(date, log(total_downloads$count))) 
```





```{r}
# working days effect 
total_downloads %>% 
  as_tsibble(index = date) %>% 
  gg_subseries(count,period = "week")
```

```{r}
# STL decomposition of total R package downloads 
total_downloads %>% 
  as_tsibble(index = date) %>% 
  model(
    STL(log(count+1) ~ trend(window=77) +
        season("week", window="periodic"))
  ) %>%
  components() %>% 
  autoplot() +
  geom_vline(xintercept = as.numeric(as.Date(UseR$date)), linetype="dotted",color = "blue", size=0.1) +
  geom_vline(xintercept = as.numeric(as.Date(Rstudio$date)), linetype="dotted",color = "red", size=0.1) 



```



```{r package-updates}
# Getting all the updates data 
library(pkgsearch)
updates <-map_dfr(cran_names[1:1000], cran_package_history)
updates_2 <- map_dfr(cran_names[1001:3000], cran_package_history)
updates_3 <- map_dfr(cran_names[3001:4000], possibly(cran_package_history, NULL))
updates_4 <- map_dfr(cran_names[4001:6000], possibly(cran_package_history, NULL))
updates_5 <- map_dfr(cran_names[6001:8000], possibly(cran_package_history, NULL))
updates_6 <- map_dfr(cran_names[8001:10000], possibly(cran_package_history, NULL))
updates_7 <- map_dfr(cran_names[10001:12000], possibly(cran_package_history, NULL))
updates_8 <- map_dfr(cran_names[12001:14000], possibly(cran_package_history, NULL))
updates_9 <- map_dfr(cran_names[14000:17828], possibly(cran_package_history, NULL))

updates_select <- c(updates,updates_2,updates_3,updates_4,updates_5,updates_6,updates_7,updates_8,updates_9)

save(updates, file = "updates.Rdata")
save(updates_2, file = "updates_2.Rdata")
save(updates_3, file = "updates_3.Rdata")
save(updates_4, file = "updates_4.Rdata")
save(updates_5, file = "updates_5.Rdata")
save(updates_6, file = "updates_6.Rdata")
save(updates_7, file = "updates_7.Rdata")
save(updates_8, file = "updates_8.Rdata")
save(updates_9, file = "updates_9.Rdata")


```


```{r}
updates_select <- updates_select %>% 
  mutate(Date=as.Date(updates_select$date))

### No. of packages in CRAN before 2010 June 
nrow(updates_select %>%  
    group_by(Package) %>% 
  filter (Date <="2010-06-30") %>% 
  filter(Version == max(Version)))

```


```{r}
# Select top 10 packages from last month 

top_downloads <- cran_top_downloads("last-month")

updates_top <- updates_select %>% 
  filter(Package %in% top_downloads$package)


downloads_top <- cran_downloads(from = "2014-01-01",to = "2021-07-05", package = top_downloads$package) 

downloads_top %>% 
  ggplot() + 
  geom_line(aes(date, count,col = package))
  


```

```{r}
# updates and ggplot2 downloads pattern 
ggplot2 <- downloads_top %>% 
  filter(package == "ggplot2") %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_top$Date, linetype="dotted", color = "red",   
             size=0.1)

 ggplotly(ggplot2,dynamicTicks = TRUE) %>%
  layout(hovermode = "x") 
```

```{r}
# STL decomposition on ggplot 
STL_ggplot <- downloads_top %>% 
  filter(package == "ggplot2") %>% 
  as_tsibble(index = date) %>% 
  model(
    STL(log(count+1) ~ trend(window=NULL) +
        season("week", window="periodic"))
  ) %>%
  components() %>% 
  autoplot() +
  geom_vline(xintercept = updates_top$Date, linetype="dotted",color = "blue", size=0.1)

ggplotly(STL_ggplot,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")

```


```{r}
# updates and dplyr downloads pattern 

dplyr <- downloads_top %>% 
  filter(package == "dplyr") %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_top$Date, linetype="dotted", color = "red",   
             size=0.1)

ggplotly(dplyr,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")
```



```{r}
# updates and tibble downloads pattern 

tibble <- downloads_top %>% 
  filter(package == "tibble") %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_top$Date , linetype="dotted", color = "red",   
             size=0.1)

ggplotly(tibble,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")
```


```{r select_pkg_ver_date}
select_pkg_ver_date <- function(x) { 
      x %>%       
        select(Package,Version,date)
}
updates_list <- 
updates_select <- map_dfr(updates_list, select_pkg_ver_date)
```




```{r}
# non-RStudio packages updates

updates_forecast <- updates_select %>% 
  filter(Package == "forecast") %>% 
  filter(Date >= as.Date("2012-10-29"))


forecast_download <- cran_downloads(packages = "forecast",from = "2012-10-29", to = "2021-07-12")

forecast <- forecast_download %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_forecast$Date , linetype="dotted", color = "red", size=0.1)+ 
  scale_x_date(limits = as.Date(c('2012-10-29','2021-07-12')))

ggplotly(forecast,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")

```


```{r}
STL_forecast <- forecast_download %>% 
  as_tsibble(index = date) %>% 
  model(
    STL(log(count+1) ~ trend(window=NULL) +
        season("week", window="periodic"))
  ) %>%
  components() %>% 
  autoplot() +
  geom_vline(xintercept = updates_forecast$Date, linetype="dotted",color = "blue", size=0.1)

ggplotly(STL_forecast,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")

```


```{r}
library(packageRank)
packageLog(package = "forecast", date = "2015-10-20")[8:14, -(4:6)]
tidyverse_2018 <- packageLog(package = "tidyverse", date = "2018-10-21")
filteredDownloads(package = "forecast", date = "2018-10-21")
filteredDownloads(package = "cholera", date = "2020-07-31")

```





```{r}
first_release <- updates_select %>% 
  group_by(Package) %>% 
  filter(Date == min(Date))

first_release %>%
  group_by(Date) %>%
  summarise(n = n_distinct(Package)) %>%
  ggplot()  + geom_line(aes(Date, n))+
  geom_smooth(aes(Date, n),stat = "smooth")


```





```{r}
library(cranlogs)
library(purrr)

n_package <-first_release %>%
  group_by(Date) %>%
  summarise(n = n_distinct(Package))

download_1 <- cran_downloads(packages= c(cran_names[1:100]),when = "last-week")
download_2 <- cran_downloads(packages= c(cran_names[101:200]),when = "last-week")
download_3 <- cran_downloads(packages= c(cran_names[201:300]),when = "last-week")

datalist <- list()

for(i in 26:175){
  Sys.sleep(sample(1:10))
  download <- cran_downloads(packages= c(cran_names[((i-1)*100+1):i*100]),when = "last-week")
  datalist[[i]] <-  download # add it to your list
}


```


```{r}

#getting the top 100 downloads for last month 

download_top100 <- get_downloads(when="last-month")

top_100 <- db %>% 
  mutate(Package = as.character(Package)) %>%
  filter(Package %in% download_top100$package )  %>%
  filter(!grepl('RStudio', Author)) %>%
  select(Package) 

# Non RStudio downloads 
top_nonrstudio <- download_top100 %>% 
  filter(package %in% top_100$Package )


updates_jsonlite <- updates_select %>% 
  filter(Package == "jsonlite") %>% 
  filter(Date >= as.Date("2012-10-29"))


jsonlite_download <- cran_downloads(packages = "jsonlite",from = "2013-12-04", to = "2021-07-12")

jsonlite <- jsonlite_download %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_jsonlite$Date , linetype="dotted", color = "red", size=0.1)+ 
  scale_x_date(limits = as.Date(c('2013-12-04','2021-07-12')))

ggplotly(jsonlite,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")

```

```{r}
updates_glue <- updates_select %>% 
  filter(Package == "glue") %>% 
  filter(Date >= as.Date("2012-10-29"))


glue_download <- cran_downloads(packages = "glue",from = "2017-04-17", to = "2021-07-12")

glue <- glue_download %>% 
  ggplot() + 
  geom_line(aes(date, count))+
  geom_vline(xintercept = updates_glue$Date , linetype="dotted", color = "red", size=0.1) + scale_x_date(limits = as.Date(c('2017-04-17','2021-07-12')))

ggplotly(glue,dynamicTicks = TRUE) %>%
  layout(hovermode = "x")
```

```{r}
updates_select %>% group_by(Package) %>% summarise(n = n(), last = max(Date)) %>% filter(n > 2) %>% arrange(desc(n))


library(aTSA)
stationary.test(jsonlite_download$count, method = "adf", nlag = NULL, type = c("Z_rho", "Z_tau"), lag.short = TRUE, output = TRUE)
# reject Null - non-stationary 


```




```{r}
library(plotly)

plot_ly(x = n_package$Date, y =  n_package$n, mode = 'lines')
```





To get the list of packages for a particular CRAN task view:

```{r ctv}
doe_pkgs <- ctv:::.get_pkgs_from_ctv_or_repos("ExperimentalDesign", 
                                              repos = "http://cran.rstudio.com/")[[1]]
survey_pkgs <- ctv:::.get_pkgs_from_ctv_or_repos("OfficialStatistics", 
                                              repos = "http://cran.rstudio.com/")[[1]]

```


```{r}
update_freq <- updates_select %>% 
  group_by(Package) %>% 
  arrange(Date) %>%
  mutate(diff = c(NA,diff(Date)))

update_avg <- update_freq %>% 
  select(Package,diff) %>% 
  na.omit() %>% 
  group_by(Package) %>%
  summarise(avg=mean(diff))

 updates_select%>% 
  filter(Package=="Ecdat")

summary(update_freq$diff)

```







How to get the package updates:

```{r pkg-updates}
pkg_url <- "https://cran.r-project.org/web/packages/{pkg}/index.html"
pkg_archive <- "https://cran.r-project.org/src/contrib/Archive/{pkg}/"

pkgs_of_interest <- doe_pkgs[1:4]
pkg_updates <- map(pkgs_of_interest, function(pkg) {
    last_update <- read_html(glue(pkg_url)) %>% 
      html_table() %>% 
      .[[1]] %>% 
      filter(X1=="Published:") %>% 
      pull(X2) %>% 
      ymd()
      
    archive_dates <- tryCatch({ 
        read_html(glue(pkg_archive)) %>% 
          html_table() %>%
          .[[1]] %>% 
          pull(`Last modified`) %>% 
          ymd_hm() %>% 
          na.omit() %>% 
          as.Date()
      }, error = function(e) {
        NULL
      })
    c(archive_dates, last_update)
  })
names(pkg_updates) <- pkgs_of_interest

updates <- unlist(pkg_updates) %>% 
  enframe("package", "update") %>% 
  # unlist converts date to integers
  mutate(update = as.Date(update, origin = "1970-01-01"),
         # need to get rid of the numbers appended to pkg names
         package = str_extract(package, paste0(pkgs_of_interest, collapse="|"))) 

updates
```



```{r}
#dependency analysis 
depends <- sapply(strsplit(as.character(db$Depends) , " \\(") , "[" , 1)
cols <- c("dep","pkg")
depends <- depends %>% 
  as.data.frame() %>% 
  mutate(pkg = db$Package )

colnames(depends) <- cols

library(devtools)
install_github("cbail/textnets")
library(textnets)
library(ggraph)
```

```{r}
dep_group <- depends %>% group_by(pkg) %>% slice(1L)
dep_cloud <- textnets::PrepText(dep_group, groupvar = "pkg", textvar = "dep", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)

dep_text_network <- CreateTextnet(dep_cloud)

VisTextNet(dep_text_network)


```
```{r}
# cleaning data 

# identify bot downloads 

bot_2014 <- spilk_2014 %>% 
  group_by(country,ip_id,package,r_version,size) %>% 
  count()

bot_2018 <- spilk_2018 %>% 
  group_by(country,ip_id,package,r_version,size,time) %>% 
  count()


entries_2012_1001_dir <- download_RStudio_CRAN_data(START = '2012-10-01',END = '2012-10-01', log_folder="/Users/daidanyang/Documents/GitHub/paper-cran-category-classification/paper/Data")

# read .gz compressed files form local directory
entries_2012_1001 <- read.csv("~/Documents/GitHub/paper-cran-category-classification/paper/Data/2014-11-17.csv.gz")


bot_2012_1001 <- entries_2012_1001 %>% 
  group_by(country,ip_id,package,r_version,size) %>% 
  count()



entries_2012_1002_dir <- download_RStudio_CRAN_data(START = '2012-10-02',END = '2012-10-02', log_folder="/Users/daidanyang/Documents/GitHub/paper-cran-category-classification/paper/Data")

# read .gz compressed files form local directory
entries_2012_1002 <- read.csv("~/Documents/GitHub/paper-cran-category-classification/paper/Data/2012-10-02.csv.gz")


bot_2012_1002 <- entries_2012_1002 %>% 
  group_by(country,ip_id,package,r_version,size) %>% 
  count()

entries_2021_0701_dir <- download_RStudio_CRAN_data(START = '2021-07-01',END = '2021-07-01', log_folder="/Users/daidanyang/Documents/GitHub/paper-cran-category-classification/paper/Data")

# read .gz compressed files form local directory
entries_2021_0701 <- read.csv("~/Documents/GitHub/paper-cran-category-classification/paper/Data/2021-07-01.csv.gz")

bot_2021_0701 <- entries_2021_0701 %>% 
  group_by(country,ip_id,package,r_version,size,time) %>% 
  count()



```

